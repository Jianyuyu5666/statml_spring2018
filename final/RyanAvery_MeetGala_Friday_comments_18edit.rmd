---
title: "2016 Election Analysis"
date: "Due June 13, 2018, midnight"
output:
  html_document: default
  pdf_document: default
editor_options: 
  chunk_output_type: console
---


# Instructions and Expectations

- You are allowed and encouraged to work with one partner on this project.  Include your names, perm numbers, and whether you are taking the class for 131 or 231 credit.

- You are welcome and encouraged to write up your report as a research paper (e.g. abstract, introduction, methods, results, conclusion) as long as you address each of the questions below.  Alternatively, you can format the assignment like a long homework by addressing each question in parts.

- There should be no raw R _output_ in the paper body!  All of your results should be formatted in a professional and visually appealing manner. That means, Either as a polished visualization or for tabular data, a nicely formatted table (see the documentation for [kable and kableExtra packages](https://haozhu233.github.io/kableExtra/awesome_table_in_pdf.pdf). If you feel you must include extensive raw R output, this should be included in an appendix, not the main report.  

- All R code should be available from your Rmarkdown file, but does not necssarily need to be shown in the body of the report!  Use the chunk option `echo=FALSE` to exclude code from appearing in your write up.  In addition to your Rmarkdown, you should turn in the write up as either a pdf document or an html file (both are acceptable).

- Many of the questions in this project are intentionally vague.  Make sure you always justify the choices you make (e.g. ''we decided to standardize the variables before classifying because ...'').  Feel free experiment and be creative!

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

indent1 = '    '
indent2 = paste(rep(indent1, 2), collapse='')
indent3 = paste(rep(indent1, 3), collapse='')

doeval = TRUE
doecho = FALSE

library(knitr)
library(tidyverse)
library(kableExtra)
library(ggmap)
library(maps)
library(Rtsne)
library(NbClust)
library(tree)
library(maptree)
library(class)
library(reshape2)
library(glmnet)
```



# Background

Predicting voter behavior is complicated for many reasons despite the tremendous effort in collecting, analyzing, and understanding many available datasets. 
For our final project, we will analyze the 2016 presidential election dataset.

The presidential election in 2012 did not come as a surprise to most. Many analysts predicted the outcome of the election correctly including [Nate Silver](https://en.wikipedia.org/wiki/Nate_Silver).  There has been some [speculation about his approach](https://www.theguardian.com/science/grrlscientist/2012/nov/08/nate-sliver-predict-us-election).

Despite largely successful predictions in 2012, the 2016 presidential election was
[more surprising](https://fivethirtyeight.com/features/the-polls-missed-trump-we-asked-pollsters-why/).  

Answer the following questions in one paragraph for each.

1. What makes predicting voter behavior (and thus election forecasting) a hard problem?

There are many challenges but perhaps the largest is nonresponse bias. In the case of elections there can be many sources of nonrepsonse bias with consequences that result in understimated predictions for either side. People that don't like answering phone calls from people they don't know (pollsters) will likely not respond to a poll call. Another source of nonresponse is that people who don't want to divulge their support for a candidate will not respond to a poll. Another challenge is that polls are expensive and to reduce cost, liberties may be taken with the methodology such as collecting lower than optimal smaple sizes or asking fewer questions to inform the analysis. Another big challenge with polling is that the polls can only ask about intended voting behavior, they can never measure how people will actually vote on election day. People change their mind on who to support due to political events (Comey's letter to Congress detailing a continued investigation into Hillary Clinton's emails), or they change their minds on whether it is worth going to vote due to work and other responsibilities. Polls are inherently difficult for prediction

2. Although Nate Silver predicted that Clinton would win 2016, [he gave Trump higher odds than most](http://fivethirtyeight.com/features/why-fivethirtyeight-gave-trump-a-better-chance-than-almost-anyone-else/). What is unique about Nate Silver's methodology?

From "The Polls Missed Trump. We Asked Pollsters Why." the authors found that Trump outperformed poll predictions more where there were more whites without a college degree.. It's possible other pollsters gave even less weight to this that NAte Silver and that this was one reaso they underperformed. The Five Thirty 8 article also mentions how they adjust for historical systemati bias in polling averages since 1972. From Nate Silver's Model Talk Podcast the day after election day, he identifies that there are three primary differences between his Polls+model and other models. A big one is that Nate silver gave more wieght to the scenario that undecided voters in midwest states would break to trump more than clinton, giving trump the electoral college win but not the popular vote. Other models did not weight this scenario as likely as Nate's did. 

3. Discuss why analysts believe predictions were less accurate in 2016.  Can anything be done to make future predictions better? What are some challenges for predicting future elections? How do you think journalists communicate results of election forecasting models to a general audience?

Nate Silver posited that he might allow more flexible priors to be incorporated into the model, such as approval ratings, or (find other example). However he says he wouldn't tinker with it too much because his model did outperform other models by a wide margin and the probability (28-30% near election day) was quite a large probability for Trump even though it still picked Clinton as the favorite. Some ideas we have is that the underlying data from the polls could be improved as Nate Silver expressed that their philosophy in constructing the model was that the polls are wrong and that this error can have structure (such as Clinton underperforming Midwest polls) that can have consequences in election outcome. Higher quality polls that make samples truly random or better account for the error of the poll due to nonresponse bias or undecided voters could improve forecast outputs. Five Thirty Eight, for example, sent journalists to Midwestern swing states where they suspected polling errors may be high due to nonrepsonse bias and a high amount of undecided voters to do their own investigation. In August 2016, Model Talk (the Five Thirty Eight podcast) highlighted that a highly rated poll from the firm Ipso had change dit's questionare mid-election to change an option "I support Neither/Other" to just "Other". In a future election, polls can do better by sticking with a single questionare format throughout, which will help modelers like Nate Silver not have to correct for inconsistent inputs to the model. This case also highlights a challenge in that Ipso changed it's questionare because they thought that repsondents were over responding to the "Other/Neither" option because of the highly polarized nature of the contest and the negative assocations with each candidate. They felt that voters who might otherwise break toward Clinton and Trump would choose "Neither/Other" more so than just "Other" and hoped to get a clear candidate choice from them isntead by changing the option. Model talk criticized this choice as it was unclear if this decision would bias towards Trump or Clinton nor was it clear if the previous option was biased toward one candidate or the other, they argue that the inconsistency is not worth the reward. Not enough direct calling polls, small sample sizes is also a factor that could be improved.

# Data

```{r data}
election.raw = read.csv("data/election/election.csv") %>% as.tbl
census_meta = read.csv("data/census/metadata.csv", sep = ";") %>% as.tbl
census = read.csv("data/census/census.csv") %>% as.tbl
census$CensusTract = as.factor(census$CensusTract)
```

## Election data

Following is the first few rows of the `election.raw` data:

```{r, echo=FALSE}
kable(election.raw %>% head(3))
```

The meaning of each column in `election.raw` is clear except `fips`. The acronym is short for [Federal Information Processing Standard](https://en.wikipedia.org/wiki/FIPS_county_code).

In our dataset, `fips` values denote the area (US, state, or county) that each row of data represent: i.e., some rows in `election.raw` are summary rows. These rows have `county` value of `NA`. There are two kinds of summary rows:

* Federal-level summary rows have `fips` value of `US`.
* State-level summary rows have names of each states as `fips` value.

## Census data

Following is the first few rows of the `census` data:

```{r, echo=FALSE}
kable(census %>% head, "html")  %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width=FALSE) %>% scroll_box(width = "100%")
```

### Census data: column metadata

Column information is given in `metadata`.

```{r, dependson=data, echo=FALSE}
kable(census_meta)
```

## Data wrangling
4. Remove summary rows from `election.raw` data: i.e.,

    * Federal-level summary into a `election_federal`.
    
    * State-level summary into a `election_state`.
    
    * Only county-level data is to be in `election`.

```{r}
election_federal = dplyr::filter(election.raw, fips=='US')
election_state = dplyr::filter(election.raw, fips!='US' & is.na(county))
election_county = dplyr::filter(election.raw, !is.na(county))
```

5. How many named presidential candidates were there in the 2016 election? Draw a bar chart of all votes received by each candidate

```{r}
election_federal = election_federal[order(election_federal$votes),]
length(unique((election.raw$candidate)))
by_candidate <- election.raw %>% group_by(candidate) %>% summarise(total_votes = sum(votes))

ggplot(data = election_federal, aes(x=reorder(candidate, -votes), y=votes)) + 
  geom_bar(stat="identity") + 
  labs(title='Candidate National Votes') +
  ylab('Vote Count') +
  theme(axis.text.x= element_text(angle=-80, size = 7))
```

6. Create variables `county_winner` and `state_winner` by taking the candidate with the highest proportion of votes. 
  Hint: to create `county_winner`, start with `election`, group by `fips`, compute `total` votes, and `pct = votes/total`. 
  Then choose the highest row using `top_n` (variable `state_winner` is similar).
  
```{r}
county_winner = election_county %>% 
  dplyr::group_by(fips) %>% 
  dplyr::mutate(total = sum(votes)) %>% 
  dplyr::mutate(pct = votes/total) %>% 
  top_n(n=1,pct)

state_winner = election_state %>% 
  dplyr::group_by(fips) %>% 
  dplyr::mutate(total = sum(votes)) %>% 
  dplyr::mutate(pct = votes/total) %>% 
  top_n(n=1,pct)
```
Visualization is crucial for gaining insight and intuition during data mining. We will map our data onto maps.

The R package `ggplot2` can be used to draw maps. Consider the following code.

```{r, message=FALSE}
states = map_data("state")
county = map_data("county")
ggplot(data = states) + 
  geom_polygon(aes(x = long, y = lat, fill = region, group = group), color = "white") + 
  coord_fixed(1.3) +
  guides(fill=FALSE)  # color legend is unnecessary and takes too long
```

The variable `states` contain information to draw white polygons, and fill-colors are determined by `region`.

7. Draw county-level map by creating `counties = map_data("county")`. Color by county
```{r}


ggplot(data = county) + 
  geom_polygon(aes(x = long, y = lat, fill = subregion, group = group), color = "white") + 
  coord_fixed(1.3) +
  guides(fill=FALSE)  # color legend is unnecessary and takes too long
```


8. Now color the map by the winning candidate for each state. 
  First, combine `states` variable and `state_winner` we created earlier using `left_join()`. 
  Note that `left_join()` needs to match up values of states to join the tables; however, they are in different formats: e.g. `AZ` vs. `arizona`.
  Before using `left_join()`, create a common column by creating a new column for `states` named
  `fips = state.abb[match(some_column, some_function(state.name))]`. 
  Replace `some_column` and `some_function` to complete creation of this new column. Then `left_join()`.
  Your figure will look similar to state_level [New York Times map](https://www.nytimes.com/elections/results/president).

```{r}
states = map_data("state")

states$fips=state.abb[match(states$region, cbind(tolower(state.name),state.abb))]
state_winner$fips <- as.factor(state_winner$fips)
states<-left_join(states,state_winner,by='fips',fixed=TRUE)

ggplot(data = states ) + 
  geom_polygon(aes(x = long, y = lat, fill = candidate, group = group), color = "white") + 
  coord_fixed(1.3) +
  guides(fill=FALSE)

```


9. The variable `county` does not have `fips` column. So we will create one by pooling information from `maps::county.fips`.
  Split the `polyname` column to `region` and `subregion`. Use `left_join()` combine `county.fips` into `county`. 
  Also, `left_join()` previously created variable `county_winner`. 
  Your figure will look similar to county-level [New York Times map](https://www.nytimes.com/elections/results/president).


```{r}
#changed to county_winner2 for debugging
county.fips<- maps::county.fips
county.fips$fips<-as.factor(county.fips$fips)
for (i in 1:length(county.fips$polyname)){
  k=strsplit(as.character(county.fips$polyname[i]), ',', fixed=TRUE)[[1]]
  county.fips$region[i] = k[1]
  county.fips$subregion[i] = k[2]
  rm(k)
}

county.fips<-left_join(county.fips,county_winner,by='fips')

counties = map_data("county")
counties = left_join(counties,county.fips,by=c('region','subregion'))

ggplot(data = counties) + 
  geom_polygon(aes(x = long, y = lat, fill = candidate, group = group), color = "white", size = .1) + 
  coord_fixed(1.3) +
  guides(fill=FALSE)  # color legend is unnecessary and takes too long
```

  
10. Create a visualization of your choice using `census` data. Many exit polls noted that 
    [demographics played a big role in the election](https://fivethirtyeight.com/features/demographics-not-hacking-explain-the-election-results/).
    Use [this Washington Post article](https://www.washingtonpost.com/graphics/politics/2016-election/exit-polls/) 
    and [this R graph gallery](https://www.r-graph-gallery.com/) for ideas and inspiration.
    
    
    ```{r}
county = map_data("county")
census_by_county = read.csv("data/census/acs2015_county_data.csv") %>% as.tbl
names(census_by_county)[names(census_by_county) == 'County'] <- 'subregion'
names(census_by_county)[names(census_by_county) == 'State'] <- 'region'
census_by_county <- census_by_county[!(census_by_county$region %in% c("alaska","hawaii","puerto rico")),]  
census_by_county = census_by_county %>% 
  mutate(subregion = tolower(subregion)) %>%
  mutate(region = tolower(region))
    ```
    
    ```{r}
# maybe helpful: https://stackoverflow.com/questions/23714052/ggplot-mapping-us-counties-problems-with-visualization-shapes-in-r
county_census = left_join(county, census_by_county, by = c("region","subregion"), fixed=TRUE)
ggplot(data = county_census) + 
  geom_polygon(aes(x = long, y = lat, fill = Black, group = group), color = "grey", size = .1) + 
  coord_fixed(1.3) +
  guides(fill="colorbar") +
  ggtitle('Percenteage of Population that is Black by County')
  
```
    
    
    
    
11. The `census` data contains high resolution information (more fine-grained than county-level).  
    In this problem, we aggregate the information into county-level data by 
    computing `TotalPop`-weighted average of each attributes for each county. Create the following variables:
    
    * _Clean census data `census.del`_: 
      start with `census`, filter out any rows with missing values, 
      convert {`Men`, `Employed`, `Citizen`} attributes to a percentages (meta data seems to be inaccurate), 
      compute `Minority` attribute by combining {Hispanic, Black, Native, Asian, Pacific}, remove {`Walk`, `PublicWork`, `Construction`}.  
      _Many columns seem to be related, and, if a set that adds up to 100%, one column will be deleted._  
      ### drop male or female for classification
```{r}
census.del = census[complete.cases(census),] %>% 
  mutate(Men = Men / TotalPop, Employed = Employed / TotalPop, Citizen = Citizen / TotalPop) %>% 
  mutate(Minority = Hispanic + Black + Native + Asian + Pacific) %>% 
  select(-Walk, -PublicWork, -Construction)

```

    * _Sub-county census data, `census.subct`_: 
      start with `census.del` from above, `group_by()` two attributes {`State`, `County`}, 
      use `add_tally()` to compute `CountyTotal`. Also, compute the weight by `TotalPop/CountyTotal`.
    
```{r}
census.subct = census.del %>% 
  group_by(State,County) %>% 
  add_tally(TotalPop)

names(census.subct)[names(census.subct) == 'n'] <- 'CountyTotal'

census.subct = census.subct %>% 
  mutate("weight" = TotalPop/CountyTotal)
census.subct = census.subct %>% select(-one_of("Women")) %>% 
  mutate_at(vars(c(Men:Citizen),c(Poverty:Minority)),funs(.*weight))
```


    * _County census data, `census.ct`_: 
      start with `census.subct`, use `summarize_at()` to compute weighted sum
    
    
```{r}

census.ct = census.subct %>% 
   group_by(State,County) %>% 
  summarize_at(vars(c(Men:Minority)),funs(sum(.)))

#for classifcation, colinearity between minority groups and White may pose issue.

```

```{r}
census.ct = census.subct %>% 
   group_by(State,County) %>% 
  summarize_at(vars(c(c(Men:Citizen),c(Poverty:Minority))),funs(sum(.*weight))) 
```

    * _Print few rows of `census.ct`_: 
    

# Dimensionality reduction

12. Run PCA for both county & sub-county level data. Save the first two principle components PC1 and PC2 into a two-column data frame, call it `ct.pc` and `subct.pc`, respectively. Discuss whether you chose to center and scale the features before running PCA and the reasons for your choice.  What are the features with the largest absolute values in the loadings matrix?

I chose to scale the variables to unit variance because the units are not uniform. Some variables in percentages are expressed as 39.0 vs .39. Some variables are note percentages and are instead counts, like the Income variables. Scaling to unit variance will normalize these differences in units. 

It appears that the first principal component loadings for Gender (Men), employment related variables (Employed, Office, and PrivateWork), and the proportion of the county that are citizens (Citizen), explained most of the varianc, though overall the loadings are similar across many different variables in the first principal component. This is true for both the county level and the subcounty level. However, the second principal component loadings for income related variables are much higher at the sub-county level that other variables' loadings. At the county level, Minority, Unemployment, Black, Native, White are the variables with the highest loadings in the second principal component. White and Minority are highly colinear, but because the minority class derived from the census does not capture all ethnicities, we chose to keep it in our analysis. 

```{r}
subct.out = census.subct %>% 
  ungroup %>% 
  select(c(Men:Minority)) %>% 
  prcomp(scale = TRUE)

ct.out = census.ct %>% 
  ungroup %>% 
  select(c(Men:Minority)) %>% 
  prcomp(scale= TRUE)
#need to change var names here
subct.pc = as.data.frame(abs(subct.out$rotation[,1:2]))
ct.pc = as.data.frame(abs(ct.out$rotation[,1:2]))

kable(subct.pc[order(abs(subct.out$rotation[,1]), decreasing=TRUE),], col.names = c("Ordered Sub County Highest Loadings for PC1", "PC2"))

kable(subct.pc[order(abs(subct.out$rotation[,2]), decreasing=TRUE),], col.names = c("PC1", "Ordered Sub County PC2"))

kable(ct.pc[order(abs(ct.out$rotation[,1]), decreasing=TRUE),], col.names = c("Ordered County Highest Loadings for PC1", "PC2"))

kable(ct.pc[order(abs(ct.out$rotation[,2]), decreasing=TRUE),], col.names = c("PC1", "Ordered County PC2"))

```

13. Determine the number of minimum number of PCs needed to capture 90% of the variance for both the county and sub-county analyses. Plot proportion of variance explained (PVE) and cumulative PVE for both county and sub-county analyses.

```{r}
var_var = subct.out$sdev^2
pve = var_var/sum(var_var)

plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained",ylim= c(0,1), type='b')
title('PVE Plot for Sub County')

plot(cumsum(pve), xlab="Principal Component ",
ylab=" Cumulative Proportion of Variance Explained ", ylim=c(0,1), type='b')
title('PVE Cumulatve for Sub County')

min(which(cumsum(pve) > .9))
print("after the 10th Principal Component, 90 percent of the variance is explained at the sub-county level")
```

```{r}
var_var = ct.out$sdev^2
pve = var_var/sum(var_var)

plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained",ylim= c(0,1), type='b')
title('PVE Plot for County')

plot(cumsum(pve), xlab="Principal Component ",
ylab=" Cumulative Proportion of Variance Explained ", ylim=c(0,1), type='b')
title('PVE Cumulatve for County')

min(which(cumsum(pve) > .9))
print("after the 8th Principal Component, 90 percent of the variance is explained at the county level")
```

# Clustering

14. With `census.ct`, perform hierarchical clustering with complete linkage.  Cut the tree to partition the observations into 10 clusters. Re-run the hierarchical clustering algorithm using the first 5 principal components of `ct.pc` as inputs instead of the original features.  Compare and contrast the results. For both approaches investigate the cluster that contains San Mateo County. Which approach seemed to put San Mateo County in a more appropriate clusters? Comment on what you observe and discuss possible explanations for these observations.
```{r}
census.ct =ungroup(census.ct)
census.ct$clust2 = NULL
census.ct$clust = NULL

census.ct.dist = dist(select(census.ct,-one_of(c("State","County"))))

set.seed(1)
census.ct.hclust = hclust(census.ct.dist)
```



```{r}
groups.10 = cutree(census.ct.hclust,10)
census.ct$clust = groups.10
sanmateo_group = filter((census.ct),clust==filter((census.ct),County=="San Mateo")$clust[1])
colnames(sanmateo_group)[1]<-"region"
colnames(sanmateo_group)[2]<-"subregion"
sanmateo_group$region<-as.character(sanmateo_group$region)
sanmateo_group$subregion<-as.character(sanmateo_group$subregion)
sanmateo_group$clust<-as.integer(sanmateo_group$clust)
```

```{r}
census_pc5<-ct.out$x[,1:5]
```

```{r}
census.pc.dist = dist(census_pc5)

set.seed(1)
census.pc.hclust = hclust(census.pc.dist)
```

```{r}
groups.pc.10 = cutree(census.pc.hclust,10)
census.ct$clust2 = groups.pc.10
sanmateo_group2 = filter((census.ct),clust2==filter((census.ct),County=="San Mateo")$clust2[1])
colnames(sanmateo_group2)[1]<-"region"
colnames(sanmateo_group2)[2]<-"subregion"

```

```{r}
summary(sanmateo_group)

```
```{r}
summary(sanmateo_group2)
```

I think that the hierarchical clustering model ((on the original features)) worked better than with using the principal components as Features. We see a better recognition ((of what?)), like the fact that ((what?)). ((In the White, Men, MeanCommute, Employed, and Private Work the maximum and magnitude of the range of values for these features are smaller for the model using original features compared to PCA. This means that the original features model produces more homogenous clusters which might be desirable as it might be better able to catch similarities between San Mateo county and other counties (such as lower employment.)))

# Classification

In order to train classification models, we need to combine `county_winner` and `census.ct` data.
This seemingly straightforward task is harder than it sounds. 
Following code makes necessary changes to merge them into `election.cl` for classification.

```{r}

tmpwinner = county_winner %>% ungroup %>%
  mutate(state = state.name[match(state, state.abb)]) %>%               ## state abbreviations
  mutate_at(vars(state, county), tolower) %>%                           ## to all lowercase
  mutate(county = gsub(" county| columbia| city| parish", "", county))  ## remove suffixes
tmpcensus = census.ct %>% mutate_at(vars(State, County), tolower)

election.cl = tmpwinner %>%
  left_join(tmpcensus, by = c("state"="State", "county"="County")) %>% 
  na.omit

## save meta information
election.meta <- election.cl %>% select(c(county, fips, state, votes, pct, total))

## save predictors and class labels
election.cl = election.cl %>% select(-c(county, fips, state, votes, pct, total))
```

Using the following code, partition data into 80% training and 20% testing:
```{r}
set.seed(10) 
n = nrow(election.cl)
in.trn= sample.int(n, 0.8*n) 
trn.cl = election.cl[ in.trn,]
tst.cl = election.cl[-in.trn,]
```

Using the following code, define 10 cross-validation folds:
```{r}
set.seed(20) 
nfold = 10
folds = sample(cut(1:nrow(trn.cl), breaks=nfold, labels=FALSE))
```

Using the following error rate function:
```{r}
calc_error_rate = function(predicted.value, true.value){
  return(mean(true.value!=predicted.value))
}
records = matrix(NA, nrow=3, ncol=2)
colnames(records) = c("train.error","test.error")
rownames(records) = c("tree","logistic","Lasso")
```

## Classification

15. Decision tree: train a decision tree by `cv.tree()`. Prune tree to minimize misclassification error. Be sure to use the `folds` from above for cross-validation. Visualize the trees before and after pruning. Save training and test errors to `records` variable. Interpret and discuss the results of the decision tree analysis. Use this plot to tell a story about voting behavior in the US (remember the [NYT infographic?](https://archive.nytimes.com/www.nytimes.com/imagepages/2008/04/16/us/20080416_OBAMA_GRAPHIC.html))
    
```{r}
trn.cl = trn.cl%>%
mutate(High=as.factor(ifelse(candidate == "Donald Trump", "No", "Yes"))) %>%
select(-c(candidate,clust,clust2))

tst.cl = tst.cl%>%
mutate(High=as.factor(ifelse(candidate == "Donald Trump", "No", "Yes"))) %>%
select(-c(candidate,clust,clust2))

tree.elections.trn = tree(High~., data = trn.cl)

draw.tree(tree.elections.trn, nodeinfo=TRUE, cex =0.7)
title("Classification Tree Built on Training Set")

cv = cv.tree(tree.elections.trn, rand=folds,FUN=prune.misclass, K=nfold)

best.cv = cv$size[which.min(cv$dev)]
best.cv
```

```{r}
# Prune tree.carseats
pt.cv = prune.misclass (tree.elections.trn, best=best.cv)
# Plot pruned tree
plot(pt.cv)
text(pt.cv, pretty=0, col = "red", cex = .5)
title("Pruned tree of size 17")
```


The most effective varibles in distinguishing who would vote for Hillary is in order:

1. Whether or not they are White

2. Whether or not they use Transit.

(Left Side of the tree). This means that counties with Higher pecentage of White people voted for trump. But we see that of the people who use public transit to get to work, even White people voted for Hillary. In using this algorithm there also might be a problem with base classes. Maybe high overall White population makes it being recognized as one of the biggest deciding features. So, it doess not tell us about whether or not other races were more likely to wvote for Hillary, which is troublesonme.

One counties have been Categorized as not having a very high White population, we see that Race is still is big factor. Minorities seem to have supported Hillary Clinton more, but we must be cautios as minorites might only be living in Democrat states. 

One intersting factor is that Self emplyed people that who used transit were more likely to vote for Donald Trump. This might be because of his promises on lower Taxes.


```{r}
# Predict on train set
pred.pt.cv.train = predict(pt.cv,trn.cl, type="class")
records[1,1]=calc_error_rate(pred.pt.cv.train,trn.cl$High)

# Predict on test set
pred.pt.cv = predict(pt.cv, tst.cl, type="class")
records[1,2]=calc_error_rate(pred.pt.cv.train,tst.cl$High)
records
```

16. Run a logistic regression to predict the winning candidate in each county.  Save training and test errors to `records` variable.  What are the significant variables? Are the consistent with what you saw in decision tree analysis? Interpret the meaning of a couple of the significant coefficients.  

```{r}
glm.fit = glm(High ~ .,
data=trn.cl, family=binomial)

summary(glm.fit)



```

Yes, the Regression coefficients seem to be Agreeing with the decision tree model. We see that Minorites are more likely to vote for ((Hillary)) and Whites are more likely to vote for Trump. It does not help ((what do you mean by this? unclear.). It also has interaction factors that might need to be analyzed like maybe Employed people (Which has a very High coefficient might make a diffence since more White people are likey to be employed). We see that people who use transit are more likely to vote for Hillary and people who are self emplyed are more likely to vote for Trump.

```{r}

prob.training = predict(glm.fit, type="response")
yhat.train =as.factor(ifelse(prob.training < 0.5, "No", "Yes"))
records[2,1]=calc_error_rate(yhat.train,trn.cl$High)

tst.glm=select(tst.cl,-High)
prob.test = round(predict(glm.fit, tst.glm, type="response"),digits=2)
yhat.test  =as.factor(ifelse(prob.test < 0.5, "No", "Yes"))
records[2,2]=calc_error_rate(yhat.test,tst.cl$High)

```


17.  You may notice that you get a warning `glm.fit: fitted probabilities numerically 0 or 1 occurred`.  As we discussed in class, this is an indication that we have perfect separation (some linear combination of variables _perfectly_ predicts the winner).  This is usually a sign that we are overfitting. One way to control overfitting in logistic regression is through regularization.  Use the `cv.glmnet` function from the `glmnet` library to run K-fold cross validation and select the best regularization parameter for the logistic regression with LASSO penalty.  Reminder: set `alpha=0` to run LASSO.  What are the non-zero coefficients in the LASSO regression for the optimal value of $\lambda$? How do they compare to the unpenalized logistic regression?   Save training and test errors to the `records` variable.
```{r}
X.trn = as.matrix(select(trn.cl,-High))
X.tst = as.matrix(select(tst.cl,-High))
y.trn = ifelse(trn.cl$High=="Yes",1,0)
cv.out.lasso=cv.glmnet(X.trn,y.trn,alpha=0,foldid = folds)
bestlam = cv.out.lasso$lambda.min
grid = 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(X.trn, y.trn, alpha=1, lambda=grid,family = "binomial")
lasso.coeff <-predict(lasso.mod, s = bestlam,newx = X.trn,type="coefficients")

```

```{r}
lasso.pred.train = predict(lasso.mod, s = bestlam,newx = X.trn,type="response")
yhat.train.lasso =as.factor(ifelse(lasso.pred.train < 0.5, "No", "Yes"))
records[3,1]=calc_error_rate(yhat.train.lasso,trn.cl$High)

lasso.pred.test = predict(lasso.mod, s = bestlam,newx = X.tst,type="response")
yhat.test.lasso  =as.factor(ifelse(lasso.pred.test < 0.5, "No", "Yes"))
records[3,2]=calc_error_rate(yhat.test.lasso,tst.cl$High)

records
```


18.  Compute ROC curves for the decision tree, logistic regression and LASSO logistic regression using predictions on the test data.  Display them on the same plot.  Based on your classification results, discuss the pros and cons of the various methods.  Are different classifiers more appropriate for answering different kinds of problems or questions?

    
```{r}
library(ROCR)
pred.linreg = prediction(prob.test, tst.cl$High)
pred.lasso = prediction(lasso.pred.test, tst.cl$High)
tpr.linreg = performance(pred.linreg, "tpr")@y.values[[1]]
fpr.linreg = performance(pred.linreg, "fpr")@y.values[[1]]
tpr.lasso = performance(pred.lasso, "tpr")@y.values[[1]]
fpr.lasso = performance(pred.lasso, "fpr")@y.values[[1]]

#plot(fpr.linreg, tpr.linreg, type="l", col=3, lwd=3, main="ROC curve")
plot(performance(pred.linreg,"tpr","fpr"),col=2,main="ROC curves")
legend(1, 95, legend=c("Linear Regression", "Lasso Regression"),
       col=c("red", "green"), lty=1:2, cex=0.8)
plot(performance(pred.lasso,"tpr","fpr"),add=TRUE,col=3,main="ROC curves")

abline(0,1)
```

The Lasso (green) definitely has worse predictive results than the logistic regression model but is far better than the  decision tree model as we see looking at the records table of test and train errors. The tree overfits on the train data and fails poorly on the test data, so I do not think that is a good model in prediction, it also does not have any interpretive results.

The Logistic Regression model might be overfiitting, but it gives us more interpretability since it does not try to push the coefficients to zero. The Lasso does, and it would be helpful in identifying which features to exculde, but I do not think that the coefficients that are non-zero should be interpretated as the one give by lasso. I think we should use Lasso to detect the features and that we should use regression for interpretation and prediction after we reduce the feature space to the features that are non zero. ((last two sentences seem contradictory. Lasso is good for feature selection but you don't agree that the non-zero coefficients should be interpreted? Also decision tree ROC is not plotted)).



# Taking it further

19. This is an open question. Interpret and discuss any overall insights gained in this analysis and possible explanations. Use any tools at your disposal to make your case: visualize errors on the map, discuss what does/doesn't seems reasonable based on your understanding of these methods, propose possible directions (collecting additional data, domain knowledge, etc).  In addition, propose and tackle _at least_ one more interesting question. Creative and thoughtful analyses will be rewarded! _This part will be worth up to a 20\% of your final project grade!  

Some possibilities for further exploration are:

  * Data preprocessing: we aggregated sub-county level data before performing classification. Would classification at the sub-county level before determining the winner perform better? What implicit assumptions are we making?

  * Exploring additional classification methods: KNN, LDA, QDA, SVM, random forest, boosting etc. (You may research and use methods beyond those covered in this course). How do these compare to logistic regression and the tree method?

  * Bootstrap: Perform bootstrap to generate plots similar to ISLR Figure 4.10/4.11. Discuss the results. 
  * Use linear regression models to predict the `total` vote for each candidate by county.  Compare and contrast these results with the classification models.  Which do you prefer and why?  How might they complement one another?
    
  * Conduct an exploratory analysis of the "purple" counties-- the counties which the models predict Clinton and Trump were roughly equally likely to win.  What is it about these counties that make them hard to predict?
  
  
    
  * Instead of using the native attributes (the original features), we can use principal components to create new (and lower dimensional) set of features with which to train a classification model.  This sometimes improves classification performance.  Compare classifiers trained on the original features with those trained on PCA features.  
    
```{r}

```

